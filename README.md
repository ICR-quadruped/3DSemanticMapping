## <b>Extending Maps with Semantic and Contextual Object Information for Robot Navigation</b> <br>[[Project Page]](https://www.verlab.dcc.ufmg.br/semantic-mapping-for-robotics/) [[Paper]](https://ieeexplore.ieee.org/document/8588525)

<img src='robotSemanticMapping/images/teaser.png' align="center" width=900>

Scene understanding is a crucial factor in the development of robots that can effectively act in an uncontrolled, dynamic, unstructured, and unknown environment, such as those found in real-world scenarios. We propose an open framework for building hybrid maps, combining both environment structure (metric map) and environment semantics (objects classes) to support autonomous robot perception and navigation tasks.

We provide in this repo the code, setup instructions and how to run the mapping system in the provided datasets. 


## (1) Installation and Setup

### Environment

Install ROS Kinetic and CUDA. 

### Configure ROS workspace (if not set already): 

> source /opt/ros/kinetic/setup.bash

> mkdir -p ~/catkin_ws/src

> cd ~/catkin_ws/

> catkin config --default-devel-space --default-build-space --default-install-space --default-source-space --cmake-args -DCMAKE_BUILD_TYPE=Release

> echo "source ~/catkin_ws/devel/setup.bash" >> ~/.bashrc

### Clone repository to src/ folder

> cd ~/catkin_ws/src

> git clone --recurse-submodules https://github.com/verlab/robotSemanticMapping.git

### Please build the packages in the following order
*Check line 23 in file "~/catkin_ws/src/darknet_ros/darknet_ros/CMakeLists.txt" to assert the cuda compile version that is compatible to [your graphics card version](https://developer.nvidia.com/cuda-gpus).*

> cd ~/catkin_ws/

> catkin build darknet_ros_msgs

> catkin build custom_msgs

> catkin build

## (2) Testing

### Offline test, using recorded robot data streams (dataset)

Download rosbag (dataset):

> mkdir _/path/to/dataset/folder_

> cd _/path/to/dataset/folder_

> wget https://www.verlab.dcc.ufmg.br/hyperlapse/downloads/turtlebot_semantic_mapping/bag_dataset.zip

### Start Test

- Terminal 1: 

> roscore

- Terminal 2: 

> rviz

_(Set configuration: File > Open Config, select rtab_mapping/rvizconfig.rviz)_

- Terminal 3:

> cd dataset

>./run_all.sh

- Terminal 4:  

> roscd auto/launch

> roslaunch yolo_detector.launch

- Terminal 5:  

> roscd auto/launch

> roslaunch obj_positioner.launch

### Online test, using physical robot

_Requirements: Kobuki base/other turtlebot base, RGBD camera, laser scan (optional) (RGBD camera depth stream can be converted to laser scan, but usually has lower range and accuracy)._

Start base: 

> roslaunch auto initialize.launch

initialize slam and yolo_detector nodes... 

**Notes**
*Before usage, check that no packages publish tf transformations, i.e., 'publish_tf' flag in launch files are set to* __false.__ *Only the rosbag play and robot description launch files should publish tf's.*

## (3) Citation & Contact

If you find this code useful for your research or the use data generated by our method, please consider citing the following papers:

	@Inproceedings{dbersan18,
	  Title          = {Semantic Map Augmentation for Robot Navigation: A Learning Approach based on Visual and Depth Data},
	  Author         = {Dhiego Bersan and Renato Martins and Mario Campos and Erickson R. Nascimento},
	  Booktitle      = {IEEE Latin American Robotics Symposium, LARS},
	  Year           = {2018}
	}
  
    @article{rmartins19,
	  Title          = {Extending Maps with Semantic and Contextual Object Information for Robot Navigation: a Learning-Based Framework using Visual and Depth Cues},
	  Author         = {Renato Martins and Dhiego Bersan and Mario Campos and Erickson R. Nascimento},
	  Booktitle      = {Journal of Intelligent and Robotic Systems (in submission)},
	  Year           = {2019}
	}

Please contact Renato Martins \<renato dot martins at dcc dot ufmg dot br\> and Dhiego Bersan \<dhiegomaga at gmail dot com\> with any comments or feedback.
